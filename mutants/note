High priority
-------------
The SSTables dont' grow as fast as I wanted.
- Inserting 500516 records. So, about 500 MB. Takes about 6 mins.
- The default record size is 1 KB. Make it 20 KB.
- Make it 20 times more bigger. Make it to 10 GB.
  from fieldcount=10, fieldlength=100
  to fieldcount=10, fieldlength=2000
    - Better fix the fieldcount. It needs a recreation of the table.
- TODO: Does a small memory node experiment reasonable? How much simulation
  speedup assumption is reasonable?

Without memory reduction, I see this big SSTables.
  $ ll *-Data.db -h
  -rw-rw-r-- 1 ubuntu ubuntu 437M Sep  6 05:02 mc-1-big-Data.db
  -rw-rw-r-- 1 ubuntu ubuntu 438M Sep  6 05:03 mc-2-big-Data.db

I don't see any read activity at all, which is what was expected.

TODO: Make Cassandra generate some read IOs.


Run experiment in the screen and detach before leaving it run. Too much
terminal output could cost a lot of money!

With 100 threads, the server seems to be saturated. Even with 50.

TODO: How do you throttle the requests?

TODO: Plot read and write latencies for different throughputs

TODO: How to trace the cql statements that the Cassandra server serves?
- This might be a close thing
  - https://www.ibm.com/support/knowledgecenter/SS3JSW_5.2.0/com.ibm.help.gdha_reference.doc/com.ibm.help.gdha_troubleshooting.doc/gdha_enabling_tracing.html
- Another idea is to look for the source code and figure out where to insert
	the statement. I think this is what I did during the development.

TODO: Want to monior CPU and disk IOs of the server



Low priority
------------
I wonder what and when ycsb writes and reads exactly. Time to look into
the code.
- Write key is incremented sequentially from recordcount. When converted to a
  string value, it is either hashed (when orderedinserts = false, the default
  value) or not. For Mutants, it doesn't mater.  What matters is the access
  popularity drop as an object ages.
- Read key generation is Zipian based on the last inserted key. With the Zipian
  constant 0.99, which I think controls how much it favors the recently
  inserted values. Don't need to understand the exact formula.  I think it
  gives a more accurate curve at the right end than my Facebook workload
  generator.
  - TODO: Would be a small interesting point in the evaluation.
  - TODO: Explore the Zipian constant

Do you need transactions? Probably not.
- For Facebook-like applications, you won't need transactions most of the time.
  - For checking out someone's latest status updates. Well, permission checks
    will need it.
- Cassandra doesn't need them. "In Cassandra, a write is atomic at the
  partition-level".
  - https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_atomicity_c.html
- TODO: What about other DBs?

TODO: Scale to 3 nodes? Probably yes. When the deployment is done.

-------------------------------------------------------------------------------

The Measurements class seems to have all latencies plus some extra JVM info.
- Do I need latencies of all operations? Or just a 10-sec summary? 10-sec
  summary might be just fine for now. If needed later, make it smaller like 1
  sec.

Too much Cassandra GCs. After restoring JVM heap size to default, I don't see
the message any more. Also, a lot higher throughput now. 40K IOPS.

What happens if you run a workload without loading?
- "workload a" gets "read-failed". It's okay.
- "workload d" gets "read-failed" too. A "run" expects some records exist
  initially. As more records are inserted, "read latest" picks recently
  inserted values more.

YCSB's _dotransactions is not a DB transaction; a group of atomic operations.
It is "Whether or not this is the transaction phase (run) or not (load).".  It
is set by default when in a "run" mode.

Entry point:
	core/src/main/java/com/yahoo/ycsb/Client.java main()

YCSB Build:
- mvn -pl com.yahoo.ycsb:cassandra-binding -am clean package -DskipTests
- Disabled the annoying checkstyle errors.

ycsb workloade d generates data on the fly.

nodetool drain seems to dump MemTable to a SSTable. You can measure the size
easily that way.  It doesn't seem to accecpt any new connections after that.

What's the network connection like between the client and the server nodes?
- Latency. ping.
	10 packets transmitted, 10 received, 0% packet loss, time 9000ms
	rtt min/avg/max/mdev = 0.412/0.458/0.496/0.028 ms
- Throughput. iperf. 1 Gbits/sec. Very stable.
- Network hops?
	traceroute from the client to the server.
		client: 54.221.47.168  10.153.161.184
		server: 54.145.43.45   10.153.211.52
	Interesting.
	- The route is pretty long.
	- I don't see any prefix of either the client or the server on the route.
	$ traceroute `cat ~/work/mutants/.run/cassandra-server-ips`
		traceroute to 54.145.43.45 (54.145.43.45), 30 hops max, 60 byte packets
		1  100.99.245.193 (100.99.245.193)  0.714 ms  1.006 ms  1.304 ms
		2  100.88.73.5 (100.88.73.5)  0.404 ms 100.88.73.10 (100.88.73.10)  0.418 ms 100.88.73.2 (100.88.73.2)  0.460 ms
		3  100.88.73.35 (100.88.73.35)  0.402 ms 100.88.73.41 (100.88.73.41)  0.394 ms 100.88.73.46 (100.88.73.46)  0.419 ms
		4  * * *
		5  100.92.134.195 (100.92.134.195)  0.323 ms 100.92.134.192 (100.92.134.192)  0.309 ms 100.92.134.32 (100.92.134.32)  0.291 ms
		6  100.92.134.18 (100.92.134.18)  0.300 ms 100.92.134.59 (100.92.134.59)  0.256 ms 100.92.134.208 (100.92.134.208)  0.284 ms
		7  100.92.133.12 (100.92.133.12)  0.271 ms 100.92.133.110 (100.92.133.110)  0.238 ms 100.92.133.248 (100.92.133.248)  0.254 ms
		8  100.92.134.155 (100.92.134.155)  0.243 ms 100.92.134.157 (100.92.134.157)  0.281 ms 100.92.134.152 (100.92.134.152)  0.239 ms
		9  100.92.134.128 (100.92.134.128)  0.358 ms 100.92.134.134 (100.92.134.134)  0.325 ms 100.92.134.141 (100.92.134.141)  0.273 ms
		10  100.92.128.132 (100.92.128.132)  0.345 ms 100.92.128.143 (100.92.128.143)  0.368 ms 100.92.128.139 (100.92.128.139)  0.324 ms
		11  100.92.128.144 (100.92.128.144)  0.337 ms 100.92.128.157 (100.92.128.157)  0.346 ms 100.92.128.153 (100.92.128.153)  0.355 ms
		12  100.92.132.235 (100.92.132.235)  0.382 ms 100.92.132.80 (100.92.132.80)  0.435 ms 100.92.132.245 (100.92.132.245)  0.352 ms
		13  100.92.128.221 (100.92.128.221)  0.382 ms 100.92.128.189 (100.92.128.189)  0.422 ms 100.92.128.177 (100.92.128.177)  0.381 ms
		14  100.92.128.163 (100.92.128.163)  0.354 ms 100.92.128.197 (100.92.128.197)  0.418 ms 100.92.128.164 (100.92.128.164)  0.354 ms
		15  100.92.240.67 (100.92.240.67)  17.670 ms 100.92.240.87 (100.92.240.87)  17.122 ms 100.92.240.139 (100.92.240.139)  15.880 ms
		16  216.182.224.114 (216.182.224.114)  19.554 ms 216.182.224.96 (216.182.224.96)  17.339 ms 216.182.224.114 (216.182.224.114)  19.513 ms
		17  ec2-54-145-43-45.compute-1.amazonaws.com (54.145.43.45)  0.430 ms  0.430 ms  0.424 ms

What is a good level of concurrency? how many threads? As long as you use the
same number for the unmodified Cassandra and Mutants, you should be fine.

How long do you want to run the workload? Long enough that multiple SSTables
are generated.

:mksession! ~/vim-session-ycsb
