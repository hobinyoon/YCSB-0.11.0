High priority
-------------
TODO: Plot latencies of workloads after shrinking the memory.




Limiting memory size, thus limiting filesystem cache size.
- Can you do it with cgroup [https://wiki.archlinux.org/index.php/cgroups]?

	cgroup
	memory.stat

- Test: Read files of specified total size sequentially in rotation and check
	if you see any slowdown from the second round.
	- TODO: fast generation of random data




TODO: Can you control the memory size for the filesystem cache?
- TODO: You can do a simple experiment and see if the performance is dropping.
- TODO: Look into what visualization vmtouch does. What are cached and what are not.

TODO: Do hypervisors cache file system blocks? Not sure. Probably not.
- TODO: Look into this. http://xenserver.org/discuss-virtualization/virtualization-blog/entry/read-caching.html
- TODO: If sharing, wow do you allocate memory to different VMs?








A read-only flat-keyed file which exists on non-root cgroups.

This breaks down the cgroup's memory footprint into different
types of memory, type-specific details, and other information
on the state and past events of the memory management system.

All memory amounts are in bytes.

The entries are ordered to be human readable, and new entries
can show up in the middle. Don't rely on items remaining in a
fixed position; use the keys to look up specific values!

anon

Amount of memory used in anonymous mappings such as
brk(), sbrk(), and mmap(MAP_ANONYMOUS)

file

Amount of memory used to cache filesystem data,
including tmpfs and shared memory.


TODO: What's the effect on reliability when you introduce multiple storages?





TODO: STCS is the default CS. Hope I can make a fair comparison with the other
databases.

I don't see any read activity at all, which is what was expected.

TODO: Make Cassandra generate some read IOs.

TODO: How to trace the cql statements that the Cassandra server serves?
- This might be a close thing
  - https://www.ibm.com/support/knowledgecenter/SS3JSW_5.2.0/com.ibm.help.gdha_reference.doc/com.ibm.help.gdha_troubleshooting.doc/gdha_enabling_tracing.html
- Another idea is to look for the source code and figure out where to insert
	the statement. I think this is what I did during the development.

TODO: Monior CPU and disk IOs of the server

TODO: Explore performance by varying the Zipian constant

TODO: Compare with the Facebook workload

TODO: Scale to 3 nodes? Probably yes. When the single-node experiment is done.

Quizup redis data
-----------------
Field format
	Timestamp
	IP addr of the client
	Command
	Parameters specific to each command

TODO: Figure out the format. Do it with the 1% of the data and ask Ymir.

Redis commands: http://redis.io/commands

TODO: check and see Ymir's dataset suggestions. See if you can find
long-running application traces.


Thoughts worth making it to the paper
-------------------------------------
Since filling up a disk takes too long -- simulating a long-running
applications --, we do a small scale experiment with the same storage to memory
ratio.
- The data request patterns follows a Zipian distribution with the access
  frequencies of the records are in the reverse order of the record age: the
  younger the record, the more frequently accessed it is. For example, the 5%
  of the youngest records get a 95% of the requests. The numbers are
  configurable with the Zipian constant.
- When you increase the number of records and the file system cache size
  proportionally, which is when you increase the disk size and RAM size
  proportionally, you should get the same file system cache miss ratio (the
  same number of disk IOs per request).
- CPU and network remain the same. They will affect the scalability of the
  system. Database IOPS.
- Memory to disk ratios of high IO instances, which are the recommended
  instance types for serving NoSQL databases.
    Model      vCPU Mem (GiB) Storage (GB) storage/mem
    i2.xlarge     4      30.5  1 x 800 SSD      26.230
    i2.2xlarge    8        61  2 x 800 SSD      26.230
    i2.4xlarge   16       122  4 x 800 SSD      26.230
    i2.8xlarge   32       244  8 x 800 SSD      26.230
    scaled down   8         2       52.459      26.230
  - TODO: You need to start from after the OS and DB server memory footprint.
    Measure and start from there!
- Increasing the record size is another way of filling up the disk fast. 20
  KB/record, for now.

TODO: explore max SSTable size and its effect on the latency vs cost trade-off.
- In general, you get a lower latency with bigger SSTable sizes and a smaller
  the number of SSTables. That's why you do SSTable compations.
- However, a coarse-grained SSTable classification gives you a less efficient
  SSTable placement, resulting in a higher storage cost under a latency SLO;
  you don't migrate SSTables as quickly as when they are smaller when they
  become cold.
- As the number of SSTables go down to a very small number, the classification
  and migration becomes harder.
- The problem becomes worse as you go down the hierarchy of the storage, since
  SSTables become bigger from compactions as they age.
  - A migration from s_n to s_(n+1) is worse than a migration from s_(n-1) to
    s_n.
- TODO: Draw a timeline diagram of SSTable migrations

Request throttling
- Although, the read / write paths are the same regardless of the load, when
  the load is heavier, the latency will be bigger.  For instance, from more IO
  contention and heavier request queue management.
- What would be the representative one, if you have to pick one? At the 50% of
  capacity? No throttling for now.
- TODO: This should be an evaulation item: scalability. How much load can a
  server sustain? How Mutants helps increase the scalability. By showing the
  throughput vs latency plot.

What and when YCSB writes and reads.
- Write key is incremented sequentially from recordcount. When converted to a
  string value, it is either hashed (when orderedinserts = false, the default
  value) or not. For Mutants, it doesn't mater.  What matters is the access
  popularity drop as an object ages.
- Read key generation is Zipian based on the last inserted key. With the Zipian
  constant 0.99, which I think controls how much it favors the recently
  inserted values. Don't need to understand the exact formula.  I think it
  gives a more accurate curve at the right end than my Facebook workload
  generator.

Do you need transactions? Probably not.
- For Facebook-like applications, you won't need transactions most of the time.
  - For checking out someone's latest status updates. Well, permission checks
    will need it.
- Cassandra doesn't need them. "In Cassandra, a write is atomic at the
  partition-level".
  - https://docs.datastax.com/en/cassandra/2.1/cassandra/dml/dml_atomicity_c.html
- TODO: What about other DBs?

The network connection between the client and the server nodes
- Latency. ping.
	10 packets transmitted, 10 received, 0% packet loss, time 9000ms
	rtt min/avg/max/mdev = 0.412/0.458/0.496/0.028 ms
- Throughput. iperf. 1 Gbits/sec. Very stable.
- Network hops?
	traceroute from the client to the server.
		client: 54.221.47.168  10.153.161.184
		server: 54.145.43.45   10.153.211.52
	Interesting.
	- The route is pretty long.
	- I don't see any prefix of either the client or the server on the route.
	$ traceroute `cat ~/work/mutants/.run/cassandra-server-ips`
		traceroute to 54.145.43.45 (54.145.43.45), 30 hops max, 60 byte packets
		1  100.99.245.193 (100.99.245.193)  0.714 ms  1.006 ms  1.304 ms
		2  100.88.73.5 (100.88.73.5)  0.404 ms 100.88.73.10 (100.88.73.10)  0.418 ms 100.88.73.2 (100.88.73.2)  0.460 ms
		3  100.88.73.35 (100.88.73.35)  0.402 ms 100.88.73.41 (100.88.73.41)  0.394 ms 100.88.73.46 (100.88.73.46)  0.419 ms
		4  * * *
		5  100.92.134.195 (100.92.134.195)  0.323 ms 100.92.134.192 (100.92.134.192)  0.309 ms 100.92.134.32 (100.92.134.32)  0.291 ms
		6  100.92.134.18 (100.92.134.18)  0.300 ms 100.92.134.59 (100.92.134.59)  0.256 ms 100.92.134.208 (100.92.134.208)  0.284 ms
		7  100.92.133.12 (100.92.133.12)  0.271 ms 100.92.133.110 (100.92.133.110)  0.238 ms 100.92.133.248 (100.92.133.248)  0.254 ms
		8  100.92.134.155 (100.92.134.155)  0.243 ms 100.92.134.157 (100.92.134.157)  0.281 ms 100.92.134.152 (100.92.134.152)  0.239 ms
		9  100.92.134.128 (100.92.134.128)  0.358 ms 100.92.134.134 (100.92.134.134)  0.325 ms 100.92.134.141 (100.92.134.141)  0.273 ms
		10  100.92.128.132 (100.92.128.132)  0.345 ms 100.92.128.143 (100.92.128.143)  0.368 ms 100.92.128.139 (100.92.128.139)  0.324 ms
		11  100.92.128.144 (100.92.128.144)  0.337 ms 100.92.128.157 (100.92.128.157)  0.346 ms 100.92.128.153 (100.92.128.153)  0.355 ms
		12  100.92.132.235 (100.92.132.235)  0.382 ms 100.92.132.80 (100.92.132.80)  0.435 ms 100.92.132.245 (100.92.132.245)  0.352 ms
		13  100.92.128.221 (100.92.128.221)  0.382 ms 100.92.128.189 (100.92.128.189)  0.422 ms 100.92.128.177 (100.92.128.177)  0.381 ms
		14  100.92.128.163 (100.92.128.163)  0.354 ms 100.92.128.197 (100.92.128.197)  0.418 ms 100.92.128.164 (100.92.128.164)  0.354 ms
		15  100.92.240.67 (100.92.240.67)  17.670 ms 100.92.240.87 (100.92.240.87)  17.122 ms 100.92.240.139 (100.92.240.139)  15.880 ms
		16  216.182.224.114 (216.182.224.114)  19.554 ms 216.182.224.96 (216.182.224.96)  17.339 ms 216.182.224.114 (216.182.224.114)  19.513 ms
		17  ec2-54-145-43-45.compute-1.amazonaws.com (54.145.43.45)  0.430 ms  0.430 ms  0.424 ms

Misc
----
Caching at the hypervisor level vs. VM level.
- "Unlike hypervisor caching, however, guest level caching is performed at the
	file level, so as a result, specific data sets can be pinned into cache right
	from the beginning to deliver an immediate cache benefit to performance
	demanding applications. Cache pinning certain data sets obviates the need to
	wait for cache warming to run its course; which can be particularly
	beneficial for applications that have hit a performance wall on conventional
	storage."
	[http://www.storage-switzerland.com/Blog/Entries/2013/8/15_Comparing_VMware_Caching_Techniques_-_Guest_vs._Hypervisor.html'
	So, I'm guessing xen is not caching blocks.

Simulation duration
- I don't think a concrete number matters, except for the Facebook workload
  experiment.

Run experiment in the screen and detach before leaving it run. Too much
terminal output could cost a lot of money!

With 100 threads, the server seems to be saturated. Even with 50. Cause they
make requests without throttling.

SSTable size growth
- The default record size is 1 KB. Make it 20 KB.
  - Inserting 500,516 records. So, about 500 MB. Takes about 6 mins.
- Make it 20 times more bigger. Make the total size to 10GB.
  from fieldcount=10, fieldlength=100 to fieldcount=10, fieldlength=2000
  - Better fix the fieldcount, otherwise you need to change the table schema.
  - Without memory capping on a c3.2xlarge node with a 15 GiB of memory.
  - Takes about 30 mins.
  - Initial sizes
    $ ll *-Data.db -h
    -rw-rw-r-- 1 ubuntu ubuntu 437M Sep  6 05:02 mc-1-big-Data.db
    -rw-rw-r-- 1 ubuntu ubuntu 438M Sep  6 05:03 mc-2-big-Data.db
  - When done. A bit less than 10 GB.
    $ ll *-Data.db -h
    -rw-rw-r-- 1 ubuntu ubuntu 6.9G Sep  6 05:30 mc-22-big-Data.db
    -rw-rw-r-- 1 ubuntu ubuntu 1.8G Sep  6 05:28 mc-26-big-Data.db
    -rw-rw-r-- 1 ubuntu ubuntu 438M Sep  6 05:25 mc-27-big-Data.db
  - The fields contain random strings. The network- or storage-level
    compression won't have a meaningful impact.

The Measurements class seems to have all latencies plus some extra JVM info.
- A 1-sec summary looks fine for now.

Too much Cassandra GCs. After restoring JVM heap size to default, I don't see
the message any more. Also, a lot higher throughput now. 40K IOPS.

What happens if you run a workload without loading?
- "workload a" gets "read-failed". It's okay.
- "workload d" gets "read-failed" too. A "run" expects some records exist
  initially. As more records are inserted, "read latest" picks recently
  inserted values more.

YCSB's _dotransactions is not a DB transaction; a group of atomic operations.
It is "Whether or not this is the transaction phase (run) or not (load).".  It
is set by default when in a "run" mode.

Entry point:
	core/src/main/java/com/yahoo/ycsb/Client.java main()

YCSB Build:
- mvn -pl com.yahoo.ycsb:cassandra-binding -am clean package -DskipTests
- Disabled the annoying checkstyle errors.

ycsb workloade d generates data on the fly.

nodetool drain seems to dump MemTable to a SSTable. You can measure the size
easily that way.  It doesn't seem to accecpt any new connections after that.

What is a good level of concurrency? how many threads? As long as you use the
same number for the unmodified Cassandra and Mutants, you should be fine.

How long do you want to run the workload? Long enough that multiple SSTables
are generated.

:mksession! ~/vim-session-ycsb
